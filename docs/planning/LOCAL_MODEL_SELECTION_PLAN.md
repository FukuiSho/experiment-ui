# ローカルモデル前提の計画（PoC→安定化→モデル選定→実験）

更新日: 2025-12-22

関連:
- ベンチマーク計画: `HIJIRI_BENCHMARK_PLAN.md`

## 1. 目的

- 実験で使うチャットモデルを **ローカルLLM（Ollama）** に統一する
- まず `cloneAI` の既存実装を活かして **動くPoC** を作る
- 安定したら **複数ローカルモデルを同一ベンチで比較**し、最も「福井聖らしい」モデルを本採用する

## 2. 全体フロー

1) PoC（動く）
- `cloneAI` がOllamaで応答できる
- UI側から叩けるAPIを用意（FastAPI）

2) 安定化（実験運用に耐える）
- 失敗時の挙動（タイムアウト/再試行/フォールバック）を決める
- セッション単位で履歴が管理できる

3) ベンチマーク作成（Limitless実会話から）
- Limitlessの会話ログから「相手発話→福井聖の返答」を抽出
- それを "期待される返答" として比較用データセット化

4) 複数モデル比較
- 同一ベンチを各モデルに流し、スコアを算出
- 上位モデルを選定（速度・安定性・本人らしさ）

5) 本採用→実験
- 選ばれたモデルを experiment-ui から利用する（/api/chat の裏をローカルに切替）

## 3. 今日の到達目標（PoC）

- `cloneAI/clone_agentAI.py` が仮想環境で落ちずに起動・応答できる
- `cloneAI/clone_server.py` を起動し、`POST /chat` が返る
- model_name を変えて呼べる（モデル選定の土台）

## 4. モデル選定工程（評価軸）

### 4.1 必須ゲート（足切り）

- **安定性**: 連続50ターンでクラッシュしない
- **レイテンシ**: 1ターン応答が許容範囲（実験の体感を損ねない）
- **再現性**: 同一プロンプトで破綻しない（極端な逸脱が少ない）

### 4.2 スコアリング（ベンチ）

Limitless実会話から作るベンチを使い、最低限以下を計測する。

- **内容一致（referenceとの近さ）**: 文字列類似（SequenceMatcher等）
- **口調・一人称**: 俺、カジュアルさ、口癖（「たしかに…」など）の傾向
- **長さ分布**: 短すぎ/長すぎのペナルティ
- **安全性**: 逸脱（暴言/差別/個人情報の不必要な露出）がない

> 統計的な厳密さは最終的に人手評価と併用推奨。まずは自動評価で足切り＆上位抽出。

## 5. 候補モデル（例）

- 軽量（速度優先）: `gemma3:1b` / `qwen2.5:1.5b` など
- 中量（品質バランス）: `qwen2.5:7b` / `llama3.1:8b` など

※ 実機スペックに合わせ、VRAM/メモリで現実的な範囲に絞る。

## 6. 実装タスク（順序）

- Step A: PoCサーバ（FastAPI）でUIから叩けるようにする
- Step B: ベンチ生成スクリプト（Limitless→JSONL）を作る
- Step C: 評価スクリプト（モデル複数→スコア→ランキング）を作る
- Step D: 上位モデルを使って experiment-ui をローカル接続に切り替える
