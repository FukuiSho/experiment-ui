# 福井聖 再現度計測ベンチマーク作成計画（Limitless→JSONL→モデル比較）

更新日: 2025-12-31

## 0. 目的（このベンチで何を決めるか）

- 目的: **Ollamaの複数ローカルモデルを同一条件で比較**し、実験で使う本採用モデル（`OLLAMA_CHAT_MODEL`）を決める。
- 対象: 「福井聖らしさ（Identity）」と「会話としての自然さ（Naturalness）」を中心に、最低限「不快さ（Offensiveness）」も足切りする。

## 1. 前提（既存実装との整合）

- 既存のベンチ生成: `cloneAI/benchmark/build_benchmark_from_limitless_md.py`
  - `limitless-knowledge.md` を解析し、(相手発話 → 福井聖返答) のペアを JSONL で出力する。
  - 形式は `experiment-ui` の `convertLimitlessToMarkdown()` が出力する `**speaker**: content` 行に整合している。
- 既存の自動評価: `cloneAI/benchmark/evaluate_models.py`
  - `SequenceMatcher` による reference 類似度
  - 返答長
  - 一人称「俺」出現率

> 方針: まずは既存スクリプトで「回る」ことを維持しつつ、精度（再現度の妥当性）を上げるためにデータ品質と指標を段階的に拡張する。

## 2. データソース設計（何をベンチにするか）

### 2.1 主データ: Limitless由来の会話

- 入力: `experiment-ui/src/data/limitless-knowledge.md`
- 期待フォーマット: 1行1発話（例）
  - `**相手**: 〜`
  - `**聖**: 〜`

### 2.2 スピーカー同定

- 福井聖側の speaker 名候補（初期）
  - `聖`, `Hijiri`, `福井聖`
- 実データに合わせて、最初の1回だけ「speaker名一覧」を目視確認して、候補を更新する（ブレると教師データが崩れる）。

## 3. ベンチデータ生成の仕様（JSONL）

### 3.1 1サンプルの定義

- サンプル条件: 
  - 直前発話が「相手」
  - 次の発話が「福井聖」
- 出力（現行互換）
  - `id`: `ex_00001` 形式
  - `prompt`: 相手発話（ユーザー入力相当）
  - `reference`: 福井聖の実返答
  - `context`: 直近の会話文脈（話者名付き）
  - `meta`: 話者情報など

### 3.2 重要なルール（再現度の精度に直結）

- **リーク回避**: `context` に `reference` 本文が含まれないこと（生成側が答えを見てしまうと評価不能）。
- **セッション境界**: `---` を越えて前後の会話を混ぜない（文脈が壊れる）。
- **低品質サンプル除外（初期案）**
  - `reference` が短すぎる（例: 2〜3文字だけ、相槌のみ）
  - URL/システムログ/ノイズ（例: タイトルや見出しのみ）
  - 明らかな文字化け

> まずは「除外ルール」を厳しくしすぎない。モデル比較では、雑ノイズが多いと差が見えなくなるので、段階的にフィルタを強化する。

## 4. データ分割（評価の信頼性を上げる）

### 4.1 基本

- `train/dev/test` の3分割を作る（モデルの学習ではなく、**評価の安定性**のための分割）。
- 分割単位は原則「セッション（見出し `# title` 〜 `---`）」。

### 4.2 推奨分割案

- `dev`: 手元でのプロンプト調整や指標改善の確認用（小さめ）
- `test`: 最終モデル選定にのみ使用（固定・変更しない）

## 5. 評価設計（何を「再現度」とみなすか）

### 5.1 自動評価（第一段階: 低コストで足切り）

最低限、以下のスコアで候補を絞る:

- **内容一致（reference近似）**
  - 現状: `SequenceMatcher` 類似度
  - 改善案: 埋め込み（Ollama embeddings）での cosine 類似（文意ベース）
- **口調・スタイル**
  - 一人称「俺」率
  - 末尾表現（例: 〜やん/〜やろ/〜っす 等）の特徴量
  - 返答の長さ分布（極端に長い/短いモデルのペナルティ）
- **安全性（最低限の足切り）**
  - 露骨な暴言/差別語のヒット（簡易ブラックリスト）
  - 個人情報（住所/電話/メール）を不必要に繰り返す傾向（ヒット率）

> 注意: 自動評価だけで「本人らしさ」を確定しない。自動はあくまで候補を絞る用途。

### 5.2 人手評価（第二段階: 研究としての妥当性）

- 被験者実験と同じ軸で、モデル選定用のミニ評価を行う:
  - Identity / Naturalness / Offensiveness（7段階など）
- 手順案:
  - `test` からランダムに N問（例: 30〜50）を抜く
  - 2〜3モデル分をブラインドで提示（モデル名は隠す）
  - 1問につき「相手発話 + 文脈」→ モデル出力 を評価

## 6. ベンチ運用（モデル比較の手順を固定する）

### 6.1 モデル候補の例（PC制約で調整）

- 軽量（速度優先）: `gemma3:1b`, `qwen2.5:1.5b`
- 中量（品質バランス）: `qwen2.5:7b`, `llama3.1:8b` など

### 6.2 比較手順（固定）

1. `limitless-knowledge.md` を最新化（必要なら）
2. ベンチJSONL生成（例: `--context-turns 3`、上限を付けるなら `--limit`）
3. `dev` でプロンプトや指標の調整（必要なら）
4. `test` を固定して各モデルを評価 → 結果JSONを保存
5. 上位2〜3モデルに絞って、ミニ人手評価 → 本採用決定
6. `experiment-ui` の `OLLAMA_CHAT_MODEL` を固定し、デモ再完走で品質確認

## 7. 再現性・提出物（研究として残す）

最低限、以下を成果物として残す:

- ベンチJSONL（`dev/test` の2種）
- 生成条件メモ（hijiri名候補、context-turns、除外ルール）
- モデル一覧（pullしたモデル名、日付）
- 自動評価結果JSON（モデル別スコア）
- 人手評価の集計（表/図）

## 8. 直近のタスク（最短の具体アクション）

- Step 1: `limitless-knowledge.md` の speaker 名一覧を確認し、`--hijiri-names` を確定
- Step 2: `--context-turns` を 1/3/5 で試し、サンプル品質（リーク/ノイズ）をざっと目視
- Step 3: `dev/test` 分割方式（セッション単位）を決めて固定
- Step 4: 候補モデル（2〜3個）で自動評価を回し、上位を選ぶ
- Step 5: 上位モデルでミニ人手評価 → 本採用
